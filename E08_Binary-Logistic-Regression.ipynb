{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Binary Logistic Regression\n",
    "Deep Learning - KI29  \n",
    "Deggendorf Institute of Technology  \n",
    "Prof. Dr. Florian Wahl"
   ],
   "id": "903ac68b585f755b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Existierende Codebase",
   "id": "f2f3c05c139446ae"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:48.693592Z",
     "start_time": "2025-05-25T13:50:47.829907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data"
   ],
   "id": "101e8466b2a31a5e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:49.171641Z",
     "start_time": "2025-05-25T13:50:49.166105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    # Initialization Code\n",
    "    def __init__(self, n_inputs, n_neurons,\n",
    "                 weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
    "                 bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
    "        # Initilalize weights and biases according to the shape given\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        # Save Regularization Lambdas\n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1\n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2\n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1\n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Calculate output as we did on the slides\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        \n",
    "        # Regularisation\n",
    "        # L1 weights\n",
    "        if self.weight_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.weights)\n",
    "            dL1[self.weights < 0] = -1\n",
    "            self.dweights += self.weight_regularizer_l1 * dL1\n",
    "            \n",
    "        # L2 weights\n",
    "        if self.weight_regularizer_l2 > 0:\n",
    "            self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
    "            \n",
    "        # L1 biases\n",
    "        if self.bias_regularizer_l1 > 0:\n",
    "            dL1 = np.ones_like(self.biases)\n",
    "            dL1[self.biases < 0] = -1\n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1\n",
    "            \n",
    "        # L2 biases\n",
    "        if self.bias_regularizer_l2 > 0:\n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
    "\n",
    "        # Gradients on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ],
   "id": "3e97da97e5b99619",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:49.260084Z",
     "start_time": "2025-05-25T13:50:49.257034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Activation_ReLU:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Calculate the output based on inputs.\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Copy before we modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        # Set to 0 if value is <=0\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ],
   "id": "c7abac5150862d5a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:49.311091Z",
     "start_time": "2025-05-25T13:50:49.307004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss:\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate the per sample loss\n",
    "        samples_losses = self.forward(output, y)\n",
    "\n",
    "        # Calculate the mean loss and return it\n",
    "        loss = np.mean(samples_losses)\n",
    "        return loss\n",
    "    \n",
    "    def regularization_loss(self, layer):\n",
    "        # Init return value to 0\n",
    "        regularization_loss = 0\n",
    "        \n",
    "        # L1 weights\n",
    "        if layer.weight_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "            \n",
    "        # L2 weights\n",
    "        if layer.weight_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights ** 2)\n",
    "            \n",
    "        # L1 biases\n",
    "        if layer.bias_regularizer_l1 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
    "            \n",
    "        # L2 biases\n",
    "        if layer.bias_regularizer_l2 > 0:\n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases ** 2)\n",
    "            \n",
    "        return regularization_loss"
   ],
   "id": "690ae03c4a1b917e",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:49.356247Z",
     "start_time": "2025-05-25T13:50:49.352726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "    def forward(self, y_pred, y_true):\n",
    "        n_samples = len(y_pred)  # Count the samples\n",
    "\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)  # Clip the predictions\n",
    "\n",
    "        # Get correct confidence values\n",
    "        # if labels are sparse\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(n_samples), y_true]\n",
    "\n",
    "        # else if labels are one hot encoded\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # Compute Losses\n",
    "        losses = -np.log(correct_confidences)\n",
    "        return losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # Number of labels in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # If labels are sparse, turn them into one-hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ],
   "id": "ff5827e55fee0aed",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:49.398146Z",
     "start_time": "2025-05-25T13:50:49.393048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Softmax activation\n",
    "class Activation_Softmax:\n",
    "    def forward(self, inputs):\n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(\n",
    "            zip(self.output, dvalues)\n",
    "        ):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(\n",
    "                single_output, single_output.T\n",
    "            )\n",
    "\n",
    "            # Calculate sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ],
   "id": "dc4abbbe5bf18a55",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:49.435801Z",
     "start_time": "2025-05-25T13:50:49.432745Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Softmax classifier - combined Softmax activation\n",
    "# and cross-entropy loss for faster backward step\n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy:\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        self.activation = Activation_Softmax()\n",
    "        self.loss = Loss_CategoricalCrossentropy()\n",
    "\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Output layer's activation function\n",
    "        self.activation.forward(inputs)\n",
    "        # Set the output\n",
    "        self.output = self.activation.output\n",
    "        # Calculate and return loss value\n",
    "        return self.loss.calculate(self.output, y_true)\n",
    "\n",
    "    def backward(self, dvalues, y_true):  # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # If labels are one-hot encoded, # turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ],
   "id": "a2bef7cf0e4b2aa0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:49.469182Z",
     "start_time": "2025-05-25T13:50:49.465087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# SGD optimizer\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, momentum=0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call before running optimization\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        if self.momentum:\n",
    "            # If layer has no momentum arrays, create them\n",
    "            if not hasattr(layer, \"weight_momentums\"):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Compute weight updates\n",
    "            weight_updates = (\n",
    "                self.momentum * layer.weight_momentums\n",
    "                - self.current_learning_rate * layer.dweights\n",
    "            )\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # Compute bias updates\n",
    "            bias_updates = (\n",
    "                self.momentum * layer.bias_momentums\n",
    "                - self.current_learning_rate * layer.dbiases\n",
    "            )\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        # Perform update\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call after running optimization\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "id": "b3bca5eaa402bd7a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:49.501426Z",
     "start_time": "2025-05-25T13:50:49.496781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer_Adagrad:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    # Call before running optimization\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer has no cache arrays, create them\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update the cache\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "\n",
    "        # Compute weight updates\n",
    "        layer.weights += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dweights\n",
    "            / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        )\n",
    "\n",
    "        # Compute bias updates\n",
    "        layer.biases += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dbiases\n",
    "            / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        )\n",
    "\n",
    "    # Call after running optimization\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "id": "8d8aa4d6fa06fd08",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:49.539887Z",
     "start_time": "2025-05-25T13:50:49.535873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer_RMSprop:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, epsilon=1e-7, rho=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    # Call before running optimization\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer has no cache arrays, create them\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update the cache\n",
    "        layer.weight_cache = (\n",
    "            self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
    "        )\n",
    "        layer.bias_cache = (\n",
    "            self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
    "        )\n",
    "\n",
    "        # Compute weight updates\n",
    "        layer.weights += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dweights\n",
    "            / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        )\n",
    "\n",
    "        # Compute bias updates\n",
    "        layer.biases += (\n",
    "            -self.current_learning_rate\n",
    "            * layer.dbiases\n",
    "            / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "        )\n",
    "\n",
    "    # Call after running optimization\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "id": "a2be070fae87ca6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:49.605341Z",
     "start_time": "2025-05-25T13:50:49.599581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Optimizer_Adam:\n",
    "    # Initialize optimizer - set settings,\n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self, learning_rate=0.001, decay=0.0, epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    # Call before running optimization\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (\n",
    "                1.0 / (1.0 + self.decay * self.iterations)\n",
    "            )\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer has no cache arrays, create them\n",
    "        if not hasattr(layer, \"weight_cache\"):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # Compute momentums\n",
    "        layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
    "        layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
    "        \n",
    "        # Perform momentum correction using beta 1 (add +1 to avoid zero-division error)\n",
    "        weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "        bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
    "\n",
    "        # Update the cache\n",
    "        layer.weight_cache = (\n",
    "            self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
    "        )\n",
    "        layer.bias_cache = (\n",
    "            self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
    "        )\n",
    "        \n",
    "        # Perform cache correction\n",
    "        weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "        bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
    "\n",
    "        # Compute weight updates\n",
    "        layer.weights += (\n",
    "            -self.current_learning_rate\n",
    "            * weight_momentums_corrected\n",
    "            / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
    "        )\n",
    "\n",
    "        # Compute bias updates\n",
    "        layer.biases += (\n",
    "            -self.current_learning_rate\n",
    "            * bias_momentums_corrected\n",
    "            / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "        )\n",
    "\n",
    "    # Call after running optimization\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ],
   "id": "9a107d5ceeb39954",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:49.637448Z",
     "start_time": "2025-05-25T13:50:49.633872Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Layer_Dropout:\n",
    "    def __init__(self, rate):\n",
    "        # Store the dropout probability which is 1-dropout_rate\n",
    "        self.rate = 1 - rate\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Save inputs\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Generate dropout mask\n",
    "        self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
    "        \n",
    "        # Apply output mask\n",
    "        self.output = inputs * self.binary_mask\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Apply gradient on values\n",
    "        self.dinputs = dvalues * self.binary_mask"
   ],
   "id": "13f77742b6d1e01c",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Sigmoid Aktivierungsfunktion",
   "id": "89666e02a94b64ee"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:49.667385Z",
     "start_time": "2025-05-25T13:50:49.664373Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        # Save inputs and calculate output\n",
    "        ...\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Calculate derivate (remember to use the \"trick\")\n",
    "        ..."
   ],
   "id": "820613e9ece119f1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:49.700279Z",
     "start_time": "2025-05-25T13:50:49.696720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Activation_Sigmoid:\n",
    "    def forward(self, inputs):\n",
    "        # Save inputs and calculate output\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        \n",
    "    def backward(self, dvalues):\n",
    "        # Calculate derivate (remember to use the \"trick\")\n",
    "        self.dinputs = dvalues * (1 - self.output) * self.output"
   ],
   "id": "2779d8ee362737a2",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Binäre Kreuzentropie",
   "id": "4848f8bb90f72828"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:49.733931Z",
     "start_time": "2025-05-25T13:50:49.730894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        ...\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        ...\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        ...\n",
    "\n",
    "        # Calculate gradient\n",
    "        ...\n",
    "        \n",
    "        # Normalize gradient\n",
    "        self.dinputs = ..."
   ],
   "id": "7b1076774a9e9130",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:49.793304Z",
     "start_time": "2025-05-25T13:50:49.789002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Binary cross-entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) +\n",
    "                          (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "\n",
    "        # Return losses\n",
    "        return sample_losses\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of outputs in every sample\n",
    "        # We'll use the first sample to count them\n",
    "        outputs = len(dvalues[0])\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / clipped_dvalues -\n",
    "                         (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ],
   "id": "ffb7deb8b8e677d6",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:51.918558Z",
     "start_time": "2025-05-25T13:50:49.850234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create dataset\n",
    "X, y = spiral_data(samples=100, classes=2)\n",
    "\n",
    "# Reshape labels to be a list of lists\n",
    "# Inner list contains one output (either 0 or 1)\n",
    "# per each output neuron, 1 in this case\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "# Create Dense layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64, weight_regularizer_l2=5e-4,\n",
    "                            bias_regularizer_l2=5e-4)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer):\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output\n",
    "# of previous layer here) and 1 output value\n",
    "dense2 = Layer_Dense(64, 1)\n",
    "\n",
    "# Create Sigmoid activation:\n",
    "activation2 = Activation_Sigmoid()\n",
    "\n",
    "# Create loss function\n",
    "loss_function = Loss_BinaryCrossentropy()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = Optimizer_Adam(decay=5e-7)#\n",
    "\n",
    "losses = []\n",
    "accuracies = []\n",
    "learning_rates = []\n",
    "\n",
    "# Train in loop\n",
    "for epoch in range(10001):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of first dense layer here\n",
    "    activation1.forward(dense1.output)\n",
    "\n",
    "    # Perform a forward pass through second Dense layer\n",
    "    # takes outputs of activation function\n",
    "    # of first layer as inputs\n",
    "    dense2.forward(activation1.output)\n",
    "\n",
    "    # Perform a forward pass through activation function\n",
    "    # takes the output of second dense layer here\n",
    "    activation2.forward(dense2.output)\n",
    "\n",
    "    # Calculate the data loss\n",
    "    data_loss = loss_function.calculate(activation2.output, y)\n",
    "    # Calculate regularization penalty\n",
    "    regularization_loss = \\\n",
    "        loss_function.regularization_loss(dense1) + \\\n",
    "        loss_function.regularization_loss(dense2)\n",
    "\n",
    "    # Calculate overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    losses.append(loss)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets\n",
    "    # Part in the brackets returns a binary mask - array consisting\n",
    "    # of True/False values, multiplying it by 1 changes it into array\n",
    "    # of 1s and 0s\n",
    "    predictions = (activation2.output > 0.5) * 1\n",
    "    accuracy = np.mean(predictions==y)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "    if not epoch % 500:\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, '+\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'data_loss: {data_loss:.3f}, ' +\n",
    "              f'reg_loss: {regularization_loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate:.8f}')\n",
    "\n",
    "    # Backward pass\n",
    "    loss_function.backward(activation2.output, y)\n",
    "    activation2.backward(loss_function.dinputs)\n",
    "    dense2.backward(activation2.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "    \n",
    "    learning_rates.append(optimizer.current_learning_rate)\n",
    "\n",
    "    # Update weights and biases\n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    optimizer.post_update_params()\n",
    "\n",
    "\n",
    "# Validate the model\n",
    "\n",
    "# Create test dataset\n",
    "X_test, y_test = spiral_data(samples=100, classes=2)\n",
    "\n",
    "# Reshape labels to be a list of lists\n",
    "# Inner list contains one output (either 0 or 1)\n",
    "# per each output neuron, 1 in this case\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Perform a forward pass of our testing data through this layer\n",
    "dense1.forward(X_test)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of first dense layer here\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Perform a forward pass through second Dense layer\n",
    "# takes outputs of activation function of first layer as inputs\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "# Perform a forward pass through activation function\n",
    "# takes the output of second dense layer here\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Calculate the data loss\n",
    "loss = loss_function.calculate(activation2.output, y_test)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets\n",
    "# Part in the brackets returns a binary mask - array consisting of\n",
    "# True/False values, multiplying it by 1 changes it into array\n",
    "# of 1s and 0s\n",
    "predictions = (activation2.output > 0.5) * 1\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')\n",
    "\n",
    "\n",
    "df = pd.DataFrame([losses, accuracies, learning_rates]).T\n",
    "df = df.rename({0: \"loss\", 1: \"accuracy\", 2: \"learning_rate\"}, axis=\"columns\")\n",
    "df.plot(subplots=True, sharex=True, sharey=False)"
   ],
   "id": "72cbcabc433590a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.500, loss: 0.693, data_loss: 0.693, reg_loss: 0.000, lr: 0.00100000\n",
      "epoch: 500, acc: 0.625, loss: 0.656, data_loss: 0.654, reg_loss: 0.002, lr: 0.00099975\n",
      "epoch: 1000, acc: 0.645, loss: 0.604, data_loss: 0.590, reg_loss: 0.014, lr: 0.00099950\n",
      "epoch: 1500, acc: 0.780, loss: 0.534, data_loss: 0.500, reg_loss: 0.034, lr: 0.00099925\n",
      "epoch: 2000, acc: 0.840, loss: 0.484, data_loss: 0.435, reg_loss: 0.049, lr: 0.00099900\n",
      "epoch: 2500, acc: 0.850, loss: 0.447, data_loss: 0.387, reg_loss: 0.060, lr: 0.00099875\n",
      "epoch: 3000, acc: 0.860, loss: 0.417, data_loss: 0.352, reg_loss: 0.066, lr: 0.00099850\n",
      "epoch: 3500, acc: 0.865, loss: 0.398, data_loss: 0.331, reg_loss: 0.067, lr: 0.00099825\n",
      "epoch: 4000, acc: 0.880, loss: 0.382, data_loss: 0.314, reg_loss: 0.068, lr: 0.00099800\n",
      "epoch: 4500, acc: 0.880, loss: 0.369, data_loss: 0.302, reg_loss: 0.068, lr: 0.00099776\n",
      "epoch: 5000, acc: 0.875, loss: 0.359, data_loss: 0.292, reg_loss: 0.067, lr: 0.00099751\n",
      "epoch: 5500, acc: 0.885, loss: 0.349, data_loss: 0.284, reg_loss: 0.065, lr: 0.00099726\n",
      "epoch: 6000, acc: 0.885, loss: 0.341, data_loss: 0.277, reg_loss: 0.064, lr: 0.00099701\n",
      "epoch: 6500, acc: 0.890, loss: 0.334, data_loss: 0.272, reg_loss: 0.062, lr: 0.00099676\n",
      "epoch: 7000, acc: 0.890, loss: 0.327, data_loss: 0.267, reg_loss: 0.060, lr: 0.00099651\n",
      "epoch: 7500, acc: 0.890, loss: 0.320, data_loss: 0.261, reg_loss: 0.059, lr: 0.00099626\n",
      "epoch: 8000, acc: 0.895, loss: 0.314, data_loss: 0.257, reg_loss: 0.057, lr: 0.00099602\n",
      "epoch: 8500, acc: 0.895, loss: 0.309, data_loss: 0.254, reg_loss: 0.056, lr: 0.00099577\n",
      "epoch: 9000, acc: 0.895, loss: 0.305, data_loss: 0.250, reg_loss: 0.054, lr: 0.00099552\n",
      "epoch: 9500, acc: 0.895, loss: 0.301, data_loss: 0.248, reg_loss: 0.053, lr: 0.00099527\n",
      "epoch: 10000, acc: 0.895, loss: 0.297, data_loss: 0.245, reg_loss: 0.052, lr: 0.00099503\n",
      "validation, acc: 0.830, loss: 0.466\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([<Axes: >, <Axes: >, <Axes: >], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:51.958854Z",
     "start_time": "2025-05-25T13:50:51.956053Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4a3fc54cd7c82983",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:52.065998Z",
     "start_time": "2025-05-25T13:50:52.063987Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "10459d95645000d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-25T13:50:52.111290Z",
     "start_time": "2025-05-25T13:50:52.108275Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bed7f1fd7e2ad869",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
